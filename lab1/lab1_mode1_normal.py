#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#  lab1.py
#  
################################################################################
# Lab 1
# =====
# Build a combination directory-bruteforcer / web crawler to enumerate a given
# URL. Third party libraries in use include requests for making the HTTP
# requests, along with requests-html to parse the HTML for links.
#
# **NOTE:** The requests-html dependency makes this script depend on Python 3.6+.
#
# Core Concepts
# -------------
# * Script structure
# * Making HTTP requests
# * Flow control (i.e. loops and exception handling)
#
# Bonus Challenges
# ----------------
# 1) Update the script to take multiple target URLs from the CLI instead of one.
# 2) Check the HTTP responses for vulnerabilities like missing / misconfigured headers.
#

import argparse
import collections
import sys
import urllib.parse
import urllib3

import requests
import requests_html

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def get_url_authority(url):
    parsed = urllib.parse.urlparse(url)
    if not parsed.scheme or not parsed.netloc:
        return None
    return parsed.scheme + '://' + parsed.netloc

def main():
    # setup the arguments to consume from the command line
    parser = argparse.ArgumentParser(description='bruteforce web directories')
    # todo: finish adding the verify_ssl (bool), verbose (bool), wordlist (str) and target_url (str) options
    args = parser.parse_args()

    # create a queue to use for directory recusion, we use collections.deque
    # for performance vs a list, and for iteration vs a queue.Queue
    url_queue = collections.deque()
    url_queue.append(  # todo: seed the queue with the correct value
    completed = collections.deque()

    # read all the lines from the word list and remove whitespace using list comprehension
    words = [word.strip() for word in args.wordlist]
    args.wordlist.close()

    while url_queue:
        # using popleft will make this list a fifo queue (first-in-first-out),
        # the first / left most item will be used
        base_url = url_queue.popleft()
        
        # make sure the base url ends with a /
        # todo: modify base url to end with a /

        for word in words:
            url = base_url + word

            # use a clean session for each request
            session = requests_html.HTMLSession()

            completed.append(url)
            # issue the request while accounting for common errors
            try:
                response = session.get(url, verify=args.verify_ssl)
            # todo: add appropriate exception handling and loop-flow control

            # check the status code from the response and print it as appropriate
            # todo: add response.status_code checks, logging and loop-flow control
            print('[+] (status: ' + str(response.status_code) + ') ' + url)

            # add the current URL to the queue to recurse into it
            if url not in url_queue and url not in completed:
                url_queue.append(url)

            for link in response.html.absolute_links:
                if get_url_authority(link) != # todo: finish the authority comparison
                    # skip links that are off-site
                    if args.verbose:
                        print('[*] skipping off-site link: ' + link)
                    continue
                if link in url_queue or link in completed:
                    # skip queued or completed urls
                    continue
                url_queue.append(link)

            # want to do something interesting with the request? because this is
            # where you'd do it
    return 0

if __name__ == '__main__':
    sys.exit(main())
